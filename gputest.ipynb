{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__) # torch 버전\n",
    "print(torch.version.cuda) # cuda 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3889,  0.0903,  0.3355,  1.4204, -1.2362],\n",
      "        [ 1.0803,  0.6316,  0.0809, -0.4822, -0.3039],\n",
      "        [ 2.7431, -0.7746, -2.1266, -0.4472,  0.8016],\n",
      "        [ 0.6726, -1.5422,  0.5993, -0.8714,  0.3406]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Bilinear 계층 정의\n",
    "bilinear = nn.Bilinear(in1_features=5, in2_features=10, out_features=3)\n",
    "\n",
    "# 입력 텐서 생성\n",
    "x1 = torch.randn(4, 5)  # 첫 번째 입력 (배치 크기: 4, 특징 차원: 5)\n",
    "x2 = torch.randn(4, 10) # 두 번째 입력 (배치 크기: 4, 특징 차원: 10)\n",
    "\n",
    "# 빌리니어 연산 수행\n",
    "output = bilinear(x1, x2)\n",
    "print(x1)\n",
    "print(output.shape)  # torch.Size([4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.5852,  0.2958],\n",
      "         [-0.2399, -0.6740]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "B = nn.Bilinear(2, 2, 1)\n",
    "print(B.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5216], grad_fn=<AddBackward0>)\n",
      "tensor([0.5216], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(B(torch.ones(2), torch.zeros(2)))\n",
    "print(B(torch.zeros(2), torch.ones(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[-0.3297, -0.3921],\n",
      "         [ 0.0159,  0.2201]]], requires_grad=True)\n",
      "['T_destination', '__annotations__', '__call__', '__class__', '__constants__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'bias', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'in1_features', 'in2_features', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'out_features', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'reset_parameters', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'weight', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "B = nn.Bilinear(2, 2, 1, bias=False)\n",
    "A = B.weight\n",
    "print(A)\n",
    "print(dir(B))\n",
    "# None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.zeros((3,1))\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b = torch.zeros(1)\n",
    "\n",
    "print(B(x_ones, x_zeros))\n",
    "# > tensor([0.], grad_fn=<ThAddBackward>)\n",
    "print(manual_bilinear(x_ones.view(1, 2), x_zeros.view(2, 1), A.squeeze(), b))\n",
    "# > tensor([0.], grad_fn=<ThAddBackward>)\n",
    "\n",
    "print(B(x_ones, x_ones))\n",
    "# > tensor([-0.7897], grad_fn=<ThAddBackward>)\n",
    "print(manual_bilinear(x_ones.view(1, 2), x_ones.view(2, 1), A.squeeze(), b))\n",
    "# > tensor([[-0.7897]], grad_fn=<ThAddBackward>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # torch 버전\n",
    "print(torch.version.cuda) # cuda 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HeavyAtomCount'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esol = ['MinEStateIndex', 'MinPartialCharge', 'MinAbsPartialCharge', 'FpDensityMorgan1', 'FpDensityMorgan2',\n",
    "        'FpDensityMorgan3', 'BCUT2D_CHGHI', 'SMR_VSA5', 'SlogP_VSA2', 'SlogP_VSA5', 'SlogP_VSA6', 'SlogP_VSA8',\n",
    "        'EState_VSA9', 'VSA_EState7', 'HeavyAtomCount', 'NumAromaticHeterocycles', 'NumHAcceptors', 'RingCount',\n",
    "        'MolMR', 'fr_C_O_noCOO']\n",
    "logvp = ['qed', 'MolWt', 'Kappa2', 'PEOE_VSA12', 'SlogP_VSA1', 'TPSA', 'EState_VSA1', 'EState_VSA10', 'VSA_EState1',\n",
    "         'VSA_EState6', 'VSA_EState8', 'HeavyAtomCount', 'NumAliphaticRings', 'NumSaturatedRings', 'fr_Ar_NH',\n",
    "         'fr_COO2', 'fr_azo', 'fr_benzene', 'fr_methoxy', 'fr_pyridine']\n",
    "\n",
    "set(esol) & set(logvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ys3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
